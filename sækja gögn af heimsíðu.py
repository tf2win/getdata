import os
import re
import requests
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin

visited_urls = set()
media_extensions = [
    # ğŸ“¹ MyndbÃ¶nd
    "mp4", "mov", "webm", "avi", "mkv", "flv", "wmv", "m4v",

    # ğŸ”Š HljÃ³Ã°
    "mp3", "wav", "ogg", "aac", "flac", "wma", "m4a",

    # ğŸ–¼ï¸ Myndir
    "jpg", "jpeg", "png", "gif", "svg", "bmp", "tiff", "webp", "ico",

    # ğŸ“„ SkjÃ¶l
    "pdf", "doc", "docx", "xls", "xlsx", "ppt", "pptx", "txt", "rtf", "odt", "ods", "odp",

    # ğŸ—ƒï¸ ÃjÃ¶ppuÃ° gÃ¶gn
    "zip", "rar", "7z", "tar", "gz", "bz2", "xz",

    # âš™ï¸ Forrit & gÃ¶gn
    "csv", "json", "xml", "yml", "ini", "log",

    # ğŸ” SkjÃ¶l meÃ° lykilorÃ°i eÃ°a dulkÃ³Ã°un
    "key", "pem", "crt", "pfx"
]
headers = {
    "User-Agent": "Mozilla/5.0"
}

def fetch_url_with_fallback(raw_url, headers=None):
    if not raw_url.startswith("http"):
        url_https = "https://" + raw_url
        url_http = "http://" + raw_url
    else:
        url_https = raw_url
        url_http = raw_url.replace("https://", "http://")

    try:
        print(f"ğŸ” PrÃ³fa: {url_https}")
        response = requests.get(url_https, headers=headers, timeout=10)
        response.raise_for_status()
        return url_https, response
    except Exception as e:
        print(f"âš ï¸  HTTPS virkaÃ°i ekki: {e}")

    try:
        print(f"ğŸ” PrÃ³fa: {url_http}")
        response = requests.get(url_http, headers=headers, timeout=10)
        response.raise_for_status()
        return url_http, response
    except Exception as e:
        print(f"âŒ HTTP virkaÃ°i ekki heldur: {e}")
        return None, None

def crawl_and_download(url, base_folder, domain):
    if url in visited_urls:
        return
    visited_urls.add(url)

    try:
        print(f"\nğŸŒ SkoÃ°a: {url}")
        res = requests.get(url, headers=headers, timeout=10)
        res.raise_for_status()
    except Exception as e:
        print(f"âŒ Gat ekki sÃ³tt {url}: {e}")
        return

    soup = BeautifulSoup(res.text, "html.parser")

    for tag in soup.find_all(["a", "source", "img", "video", "audio", "iframe", "embed"]):
        for attr in ["href", "src"]:
            if tag.has_attr(attr):
                link = tag[attr]
                full_url = urljoin(url, link)
                ext = full_url.lower().split("?")[0].split(".")[-1]

                if ext in media_extensions:
                    filename = os.path.basename(full_url.split("?")[0])
                    save_path = os.path.join(base_folder, filename)

                    try:
                        print(f"â¬‡ SÃ¦ki {filename}...")
                        with requests.get(full_url, stream=True, headers=headers, timeout=15) as r:
                            r.raise_for_status()
                            with open(save_path, "wb") as f:
                                for chunk in r.iter_content(chunk_size=8192):
                                    if chunk:
                                        f.write(chunk)
                        print(f"âœ… VistaÃ°: {save_path}")
                    except Exception as e:
                        print(f"âš  Gat ekki sÃ³tt {full_url}: {e}")

    # Leitum aÃ° fleiri tenglum Ã¡ sÃ¶mu sÃ­Ã°u og kÃ¶llum aftur Ã¡ falliÃ°
    for link_tag in soup.find_all("a", href=True):
        next_url = urljoin(url, link_tag['href'])
        if domain in next_url and next_url not in visited_urls:
            crawl_and_download(next_url, base_folder, domain)

# === AÃ°al keyrsla ===
if __name__ == "__main__":
    raw_url = input("ğŸ“¥ SlÃ¡Ã°u inn vefslÃ³Ã°: ").strip()
    url, response = fetch_url_with_fallback(raw_url, headers)

    if not response:
        print("ğŸš« Ekki tÃ³kst aÃ° nÃ¡lgast sÃ­Ã°una.")
    else:
        parsed = urlparse(url)
        domain = parsed.netloc
        safe_name = parsed.path.strip("/").replace("/", "_") or "niÃ°urhal"
        folder_name = f"niÃ°urhal_{domain}_{safe_name}"
        os.makedirs(folder_name, exist_ok=True)

        crawl_and_download(url, folder_name, domain)

        print(f"\nğŸ‰ LokiÃ°! GÃ¶gn vistuÃ° Ã­ mÃ¶ppunni: {folder_name}")
